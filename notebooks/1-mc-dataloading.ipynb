{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-10T01:22:02.045665Z",
     "start_time": "2020-04-10T01:22:02.036670Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "from collections import OrderedDict\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "\n",
    "import lineflow as lf\n",
    "import lineflow.datasets as lfds\n",
    "import lineflow.cross_validation as lfcv\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, SequentialSampler, RandomSampler\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from transformers import BertForSequenceClassification, BertTokenizer, AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-10T01:21:42.471085Z",
     "start_time": "2020-04-10T01:21:42.466160Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "MAX_LEN = 256\n",
    "NUM_LABELS = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-10T01:18:10.351436Z",
     "start_time": "2020-04-10T01:18:10.342525Z"
    }
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-10T01:31:15.868841Z",
     "start_time": "2020-04-10T01:31:15.717956Z"
    }
   },
   "outputs": [],
   "source": [
    "# creator\n",
    "# returns path of pickle or parquet\n",
    "data_path = Path('../data/external/domains/data')\n",
    "domain_files = sorted(data_path.glob('**/*.txt'))\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_domains, test_domains = train_test_split(domain_files, random_state=42, train_size=0.8)\n",
    "test_domains, dev_domains = train_test_split(test_domains, random_state=42, train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-10T01:26:10.515243Z",
     "start_time": "2020-04-10T01:26:10.507932Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4490.ventures'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "arrayfiles.TextFile(f)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-10T01:38:29.181565Z",
     "start_time": "2020-04-10T01:38:29.152264Z"
    }
   },
   "outputs": [],
   "source": [
    "# see  https://github.com/tofunlp/lineflow/blob/master/lineflow/datasets/wikitext.py\n",
    "import io\n",
    "import pickle\n",
    "from functools import lru_cache\n",
    "from typing import Dict, List, Union\n",
    "from sklearn.model_selection import train_test_split\n",
    "import arrayfiles\n",
    "from tqdm.auto import tqdm\n",
    "from lineflow import Dataset, download\n",
    "\n",
    "\n",
    "def get_domain(data_path: Path) -> Dict[str, Union[arrayfiles.TextFile, List]]:\n",
    "    \n",
    "    root = download.get_cache_directory(os.path.join('datasets', 'domain'))\n",
    "\n",
    "    data_path = Path('../data/external/domains/data')\n",
    "\n",
    "\n",
    "    def creator(path):\n",
    "        domain_files = sorted(data_path.glob('**/*.txt'))\n",
    "\n",
    "        train_domains, test_domains = train_test_split(domain_files, random_state=42, train_size=0.8)\n",
    "        test_domains, dev_domains = train_test_split(test_domains, random_state=42, train_size=0.8)\n",
    "        splits = dict(test=test_domains, train=train_domains, dev=dev_domains)\n",
    "    \n",
    "        dataset = {}\n",
    "        for split, files in splits.items():\n",
    "            dataset[split] = [arrayfiles.TextFile(f) for f in tqdm(files, desc=split)]\n",
    "        with io.open(path, 'wb') as f:\n",
    "            pickle.dump(dataset, f)\n",
    "        return dataset\n",
    "\n",
    "    def loader(path):\n",
    "        with io.open(path, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "\n",
    "    pkl_path = os.path.join(root, 'domain.pkl')\n",
    "    return download.cache_or_load_file(pkl_path, creator, loader)\n",
    "\n",
    "\n",
    "cached_get_domain = lru_cache()(get_domain)\n",
    "\n",
    "\n",
    "class Domain(Dataset):\n",
    "    def __init__(self,\n",
    "                 split: str = 'train') -> None:\n",
    "        if split not in {'train', 'test'}:\n",
    "            raise ValueError(f\"only 'train', 'dev' and 'test' are valid for 'split', but '{split}' is given.\")\n",
    "\n",
    "        raw = cached_get_domain()\n",
    "        super().__init__(raw[split])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-10T01:18:11.405275Z",
     "start_time": "2020-04-10T01:18:11.378574Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BertTokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-e4537ad17bd4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;31m# Given two sentences, x[\"string1\"] and x[\"string2\"], this function returns BERT ready inputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     inputs = tokenizer.encode_plus(\n\u001b[1;32m      4\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"string1\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"string2\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BertTokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# see convert_examples_to_features\n",
    "\n",
    "def preprocess(tokenizer: BertTokenizer, x: Dict) -> Dict:\n",
    "# Different models need different input formatting and/or extra arguments\n",
    "    requires_preprocessing = args.model_type in PREPROCESSING_FUNCTIONS.keys()\n",
    "    if requires_preprocessing:\n",
    "        prepare_input = PREPROCESSING_FUNCTIONS.get(args.model_type)\n",
    "        preprocessed_prompt_text = prepare_input(args, model, tokenizer, prompt_text)\n",
    "        encoded_prompt = tokenizer.encode(\n",
    "            preprocessed_prompt_text, add_special_tokens=False, return_tensors=\"pt\", add_space_before_punct_symbol=True\n",
    "        )\n",
    "    else:\n",
    "        encoded_prompt = tokenizer.encode(prompt_text, add_special_tokens=False, return_tensors=\"pt\"max_length=MAX_LEN,)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Given two sentences, x[\"string1\"] and x[\"string2\"], this function returns BERT ready inputs.\n",
    "    inputs = tokenizer.encode_plus(\n",
    "            x[\"string1\"],\n",
    "            add_special_tokens=False,\n",
    "            max_length=MAX_LEN,\n",
    "            )\n",
    "\n",
    "    # First `input_ids` is a sequence of id-type representation of input string.\n",
    "    # Second `token_type_ids` is sequence identifier to show model the span of \"string1\" and \"string2\" individually.\n",
    "    input_ids, token_type_ids = inputs[\"input_ids\"], inputs[\"token_type_ids\"]\n",
    "    attention_mask = [1] * len(input_ids)\n",
    "\n",
    "    # BERT requires sequences in the same batch to have same length, so let's pad!\n",
    "    padding_length = MAX_LEN - len(input_ids)\n",
    "\n",
    "    pad_id = tokenizer.pad_token_id\n",
    "    input_ids = input_ids + ([pad_id] * padding_length)\n",
    "    attention_mask = attention_mask + ([0] * padding_length)\n",
    "    token_type_ids = token_type_ids + ([pad_id] * padding_length)\n",
    "\n",
    "    # Super simple validation.\n",
    "    assert len(input_ids) == MAX_LEN, \"Error with input length {} vs {}\".format(len(input_ids), MAX_LEN)\n",
    "    assert len(attention_mask) == MAX_LEN, \"Error with input length {} vs {}\".format(len(attention_mask), MAX_LEN)\n",
    "    assert len(token_type_ids) == MAX_LEN, \"Error with input length {} vs {}\".format(len(token_type_ids), MAX_LEN)\n",
    "\n",
    "    # Convert them into PyTorch format.\n",
    "    label = torch.tensor(int(x[\"quality\"])).long()\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_mask = torch.tensor(attention_mask)\n",
    "    token_type_ids = torch.tensor(token_type_ids)\n",
    "\n",
    "    # DONE!\n",
    "    return {\n",
    "            \"label\": label,\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"token_type_ids\": token_type_ids\n",
    "            }\n",
    "\n",
    "\n",
    "def nonefilter(dataset):\n",
    "    filtered = []\n",
    "    for x in dataset:\n",
    "        if x[\"string1\"] is None:\n",
    "            continue\n",
    "        if x[\"string2\"] is None:\n",
    "            continue\n",
    "        filtered.append(x)\n",
    "    return lf.Dataset(filtered)\n",
    "\n",
    "\n",
    "def get_dataloader():\n",
    "    train = lfds.MsrParaphrase(\"train\")\n",
    "    train = nonefilter(train)\n",
    "    test = lfds.MsrParaphrase(\"test\")\n",
    "    test = nonefilter(test)\n",
    "    train, val = lfcv.split_dataset_random(train, int(len(train) * 0.8), seed=42)\n",
    "    batch_size = 8\n",
    "\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)\n",
    "    preprocessor = partial(preprocess, tokenizer)\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "            train.map(preprocessor),\n",
    "            sampler=RandomSampler(train),\n",
    "            batch_size=batch_size\n",
    "            )\n",
    "    val_dataloader = DataLoader(\n",
    "            val.map(preprocessor),\n",
    "            sampler=SequentialSampler(val),\n",
    "            batch_size=batch_size\n",
    "            )\n",
    "    test_dataloader = DataLoader(\n",
    "            test.map(preprocessor),\n",
    "            sampler=SequentialSampler(test),\n",
    "            batch_size=batch_size\n",
    "            )\n",
    "\n",
    "    return train_dataloader, val_dataloader, test_dataloader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Model(pl.LightningModule):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=NUM_LABELS)\n",
    "        self.model = model\n",
    "\n",
    "        train_dataloader, val_dataloader, test_dataloader = get_dataloader()\n",
    "        self._train_dataloader = train_dataloader\n",
    "        self._val_dataloader = val_dataloader\n",
    "        self._test_dataloader = test_dataloader\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        param_optimizer = list(self.model.named_parameters())\n",
    "        no_decay = [\"bias\", \"gamma\", \"beta\"]\n",
    "        optimizer_grouped_parameters = [\n",
    "                {\n",
    "                    \"params\": [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "                    \"weight_decay_rate\": 0.01\n",
    "                    },\n",
    "                {\n",
    "                    \"params\": [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "                    \"weight_decay_rate\": 0.0\n",
    "                    },\n",
    "                ]\n",
    "        optimizer = AdamW(\n",
    "                optimizer_grouped_parameters,\n",
    "                lr=2e-5,\n",
    "                )\n",
    "        return optimizer\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        labels = batch[\"label\"]\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        token_type_ids = batch[\"token_type_ids\"]\n",
    "\n",
    "        loss, _ = self.model(\n",
    "                input_ids,\n",
    "                token_type_ids=token_type_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "                )\n",
    "\n",
    "        tqdm_dict = {\"train_loss\": loss}\n",
    "        output = OrderedDict({\n",
    "            \"loss\": loss,\n",
    "            \"progress_bar\": tqdm_dict,\n",
    "            \"log\": tqdm_dict\n",
    "            })\n",
    "\n",
    "        return output\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        labels = batch[\"label\"]\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        token_type_ids = batch[\"token_type_ids\"]\n",
    "\n",
    "        loss, logits = self.model(\n",
    "                input_ids,\n",
    "                token_type_ids=token_type_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "                )\n",
    "        labels_hat = torch.argmax(logits, dim=1)\n",
    "\n",
    "        correct_count = torch.sum(labels == labels_hat)\n",
    "\n",
    "        if self.on_gpu:\n",
    "            correct_count = correct_count.cuda(loss.device.index)\n",
    "\n",
    "        output = OrderedDict({\n",
    "            \"val_loss\": loss,\n",
    "            \"correct_count\": correct_count,\n",
    "            \"batch_size\": len(labels)\n",
    "            })\n",
    "        return output\n",
    "\n",
    "    def validation_end(self, outputs):\n",
    "        val_acc = sum([out[\"correct_count\"] for out in outputs]).float() / sum(out[\"batch_size\"] for out in outputs)\n",
    "        val_loss = sum([out[\"val_loss\"] for out in outputs]) / len(outputs)\n",
    "        tqdm_dict = {\n",
    "                \"val_loss\": val_loss,\n",
    "                \"val_acc\": val_acc,\n",
    "                }\n",
    "        result = {\"progress_bar\": tqdm_dict, \"log\": tqdm_dict, \"val_loss\": val_loss}\n",
    "        return result\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        labels = batch[\"label\"]\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        token_type_ids = batch[\"token_type_ids\"]\n",
    "\n",
    "        loss, logits = self.model(\n",
    "                input_ids,\n",
    "                token_type_ids=token_type_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "                )\n",
    "        labels_hat = torch.argmax(logits, dim=1)\n",
    "\n",
    "        correct_count = torch.sum(labels == labels_hat)\n",
    "\n",
    "        if self.on_gpu:\n",
    "            correct_count = correct_count.cuda(loss.device.index)\n",
    "\n",
    "        output = OrderedDict({\n",
    "            \"test_loss\": loss,\n",
    "            \"correct_count\": correct_count,\n",
    "            \"batch_size\": len(labels)\n",
    "            })\n",
    "\n",
    "        return output\n",
    "\n",
    "    def test_end(self, outputs):\n",
    "        test_acc = sum([out[\"correct_count\"] for out in outputs]).float() / sum(out[\"batch_size\"] for out in outputs)\n",
    "        test_loss = sum([out[\"test_loss\"] for out in outputs]) / len(outputs)\n",
    "        tqdm_dict = {\n",
    "                \"test_loss\": test_loss,\n",
    "                \"test_acc\": test_acc,\n",
    "                }\n",
    "        result = {\"progress_bar\": tqdm_dict, \"log\": tqdm_dict}\n",
    "        return result\n",
    "\n",
    "    @pl.data_loader\n",
    "    def train_dataloader(self):\n",
    "        return self._train_dataloader\n",
    "\n",
    "    @pl.data_loader\n",
    "    def val_dataloader(self):\n",
    "        return self._val_dataloader\n",
    "\n",
    "    @pl.data_loader\n",
    "    def test_dataloader(self):\n",
    "        return self._test_dataloader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    early_stop_callback = EarlyStopping(\n",
    "            monitor=\"val_loss\",\n",
    "            min_delta=0.0,\n",
    "            patience=3,\n",
    "            verbose=True,\n",
    "            mode=\"min\"\n",
    "            )\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "            gpus=1,\n",
    "            early_stop_callback=early_stop_callback,\n",
    "            )\n",
    "\n",
    "    model = Model()\n",
    "\n",
    "    trainer.fit(model)\n",
    "    trainer.test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer-domains",
   "language": "python",
   "name": "transformer-domains"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
